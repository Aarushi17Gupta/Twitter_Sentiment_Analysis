{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1407cb",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "Sentiment Analysis is one of the application of Natural Language processing (NLP). It involves classifying a piece of text as negative,positive or neutral.\n",
    "##### The objective of this project is to recognize whether the given tweet is oriented as positive(1), negative(-1) or neutral(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633580a",
   "metadata": {},
   "source": [
    "### Step 1: Loading the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cedab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import re # for regular expressions\n",
    "import nltk # for text manipulation\n",
    "import string\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\",200)\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a24b9e",
   "metadata": {},
   "source": [
    "### Step 2: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data=pd.read_csv(\"Twitter_Data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac16ca7b",
   "metadata": {},
   "source": [
    "#### Text is a highly unstructed data with various types of noises present in it and the data cannot be analyzed properly with preprocessing. Thus data preprocessing involves first inspecting the data and then cleaning it for applying the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc939b4",
   "metadata": {},
   "source": [
    "### Step 3: Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking few positive tweets\n",
    "data[data['category']==1.0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking few neutral tweets\n",
    "data[data['category']==0.0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking few negative tweets\n",
    "data[data['category']==(-1.0)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for total number of missing values in each category\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c669499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the total data is very large compared to the missing values\n",
    "# We can drop all the rows with null values\n",
    "data.dropna(axis=0,inplace=True)\n",
    "data_cleaned=data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again checking whether null values are present or not in the datset\n",
    "data_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dimensions of our data\n",
    "data_cleaned.shape\n",
    "# There are 1,62,969 rows and 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the total number of values in each category\n",
    "data_cleaned['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec03fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of length of the tweets for both train and test data\n",
    "length_data=data_cleaned['clean_text'].str.len()\n",
    "#length_test=test['tweet'].str.len()\n",
    "sns.histplot(length_data,bins=20,label='data_tweets',color='green')\n",
    "#sns.histplot(length_test,bins=20,label='test_tweets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a5d9b",
   "metadata": {},
   "source": [
    "#### Cleaning raw data is an important step as it helps in getting rid of unwanted words and characters which helps in obtaining better features\n",
    "#### It is always better to remove special symbols such as punctuation, special characters, numbers or terms which don't carry much weightage in the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d336dca",
   "metadata": {},
   "source": [
    "### Step 4: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc33ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove all the unwanted text patterns from the tweets\n",
    "def remove_pattern(input_txt,pattern):\n",
    "    r=re.findall(pattern,input_txt)\n",
    "    for i in r:\n",
    "        input_txt=re.sub(i,\" \",input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbedaae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Althouugh the given data has no twitter handles present but it is always better to make a code\n",
    "# for it in case there are few present in it.\n",
    "# 1. Removing Twitter Handles(@user)\n",
    "data_cleaned['tidy_tweet']=np.vectorize(remove_pattern)(data_cleaned['clean_text'],\"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7ddcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Removing punctuations,numbers and special characters\n",
    "data_cleaned['tidy_tweet']=data_cleaned['tidy_tweet'].str.replace(\"[^a-zA-Z#]\",\" \",regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaea0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Removing short words since short words like 'hmm','his','ohh'... does not carry any important message\n",
    "data_cleaned['tidy_tweet']=data_cleaned['tidy_tweet'].apply(lambda x:\" \".join([w for w in x.split() if len(w)>3]))\n",
    "data_cleaned.head()\n",
    "# Below is the cleaned data which we can stored in the new column named as 'tidy_tweets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d090c825",
   "metadata": {},
   "source": [
    "### Step 5: Text Normalization\n",
    "#### Next step involves Text Normalization which involves extracting base terms from the morphological words. Before that we need to tokenize the tweets. Tokenization is the process of splitting a string of texts into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61108837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of strings. Tokens are individual terms or words\n",
    "tokenized_tweet=data_cleaned['tidy_tweet'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87027f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=nltk.wordnet.WordNetLemmatizer()\n",
    "tokenized_tweet=tokenized_tweet.apply(lambda x:[lemmatizer.lemmatize(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc511d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitching these tokens back together\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i]=' '.join(tokenized_tweet[i])\n",
    "data_cleaned['tidy_tweet']=tokenized_tweet\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98266a3c",
   "metadata": {},
   "source": [
    "### Step 6: Making Word Clouds (Visualization from tweets)\n",
    "#### Visualizing the data is an important step for story telling and gaining insights\n",
    "#### A WordCloud is a type of visualization where most frequent words appear in larger size and less frequent words in smaller size which helps in understanding the most common words used in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f9d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining all the words for all tweets\n",
    "all_words=' '.join([text for text in data_cleaned['tidy_tweet']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c55b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the wordcloud module\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b42709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the words used in negative tweets\n",
    "negative_words=\" \".join([text for text in data_cleaned['tidy_tweet'][data_cleaned['category']==(-1.0)]])\n",
    "wordcloud=WordCloud(width=800,height=500,random_state=21,max_font_size=110).generate(negative_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud,interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3cf612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the words used in positive tweets\n",
    "positive_words=\" \".join([text for text in data_cleaned['tidy_tweet'][data_cleaned['category']==1.0]])\n",
    "wordcloud=WordCloud(width=800,height=500,random_state=21,max_font_size=110,colormap=\"Blues\").generate(positive_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud,interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8a0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the  words used in neutral tweets\n",
    "neutral_words=\" \".join([text for text in data_cleaned['tidy_tweet'][data_cleaned['category']==0.0]])\n",
    "wordcloud=WordCloud(width=800,height=500,random_state=21,max_font_size=110,background_color=\"white\").generate(neutral_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud,interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30927dcb",
   "metadata": {},
   "source": [
    "### Step 7: Data Analyses\n",
    "#### To analyze the preprocessed data, it needs to be converted into features. Depending upon the usage, a text data can be constructed using assorted techniques such as Bag of words, TF-IDF and Word Embeddings\n",
    "#### Here we are using TF-IDF Features- TF-IDF means Term Frequency - Inverse Document Frequency. ... TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ee6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the module from sklearn for TF-IDF features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04529460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of the class TFIDFVectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer(max_df=0.9,min_df=2,max_features=10000,stop_words='english')\n",
    "tfidf=tfidf_vectorizer.fit_transform(data_cleaned['tidy_tweet'])\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f75b0",
   "metadata": {},
   "source": [
    "### Step 8: Modeling\n",
    "#### We will now build the model on the datasets with different feature sets prepared in the earlier sections\n",
    "#### Here we will be using the LogisticRegression algorithm for the model\n",
    "#### F1 score -- is used as the evaluation metric. It is the weighted average of Precision and Recall . Thus, this score takes both false positives and false negatives into account. \n",
    "#### Logistic Regression -- It is a special case of linear regression when the outcome variable is categorical, where we are using log of odds as the dependent variables..It also predicts the probability of occurence of an event by fitting data to a logit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f99de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the following modules from sklearn for our model building\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9490f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing set\n",
    "# We are using split size as 0.2 which means 80% od data is used for training the data and 20% for testing it\n",
    "xtrain_tfidf,xtest,ytrain_tfidf,ytest=train_test_split(tfidf,data_cleaned['category'],random_state=42,test_size=0.2)\n",
    "xtrain_tfidf.shape,xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ae68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing set\n",
    "# For cross validation we are further splitting the train data into training and validation dataset\n",
    "# here test size is set to 0.15.\n",
    "# 85% data is used for training the model and 15% for validation\n",
    "xtrain,xvalid,ytrain,yvalid=train_test_split(xtrain_tfidf,ytrain_tfidf,random_state=42,test_size=0.15)\n",
    "xtrain.shape,xvalid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6503e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "# creating an instance of the class LogisticRegression\n",
    "lreg=LogisticRegression(max_iter=500)\n",
    "lreg.fit(xtrain,ytrain)\n",
    "# Predicting the variables \n",
    "train_predict=lreg.predict(xtrain)\n",
    "# comparing the predicted values from the target values\n",
    "f1_score(ytrain,train_predict,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating our training on validation dataset before testing it on the final data\n",
    "valid_predict=lreg.predict(xvalid)\n",
    "f1_score(yvalid,valid_predict,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e217fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At last testing our data for good accuracy(here f1 score)\n",
    "test_predict=lreg.predict(xtest)\n",
    "f1_score(ytest,test_predict,average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6d6e37",
   "metadata": {},
   "source": [
    "### Step 9: Making confusion matrix \n",
    "#### Now, it's time to wrap-up things. For this we are making confusion matrix which gives an idea about true positives, true negatives, false positives and false negatives in our final score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c320c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cf_matrix=confusion_matrix(ytest,test_predict)\n",
    "cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a nicer and neater way of presenting the confusion matrix using heatmaps. \n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "          zip(group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(3,3)\n",
    "ax=sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='Blues')\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Sentiment Category')\n",
    "ax.set_ylabel('Actual Sentiment Category ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Negative','Neutral', 'Positive'])\n",
    "ax.yaxis.set_ticklabels(['Negative','Neutral', 'Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8283a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also print a nice summary giving values for precision, recall and f-score using classification report from sklearn\n",
    "class_report=classification_report(ytest,test_predict)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3550ac8e",
   "metadata": {},
   "source": [
    "# F1 score = 82%\n",
    "# Recall = 80%\n",
    "# Precision= 82%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
